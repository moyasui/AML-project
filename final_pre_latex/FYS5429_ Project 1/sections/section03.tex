\section{Results}\label{results}


\subsubsection{Regular FFNN} 

We start by investigating how well a regular feed-forward neural network is able to predict the trajectories of the Lorenz attractor. By training on 9 trajectories with different initial conditions and testing on one, we generate the graph of \figref{fig:ffn_lorenz3d}. This figure was obtained with a three-layer network, where the number of nodes in each was, respectively, 64, 32, and 3.


\begin{figure}[H]
\includegraphics[width= \linewidth]{figs/lorenz_ffn_3d.pdf}
    \caption{Predicted versus test trajectories for the FFNN on the Lorenz attractor.}
    \label{fig:ffn_lorenz3d}
\end{figure}

For the loss function in this implementation, we used the mean absolute error and an Adam optimizer with a learning rate of $10^{-3}$. The training process was of 200 epochs, and \figref{fig:history_ffn} shows how the validation loss evolves in comparison to the training set loss throughout the training. The metric used for training this network was the mean absolute error, but the $y$-axis is shown as the mean absolute error (MAE) divided by the average of the trajectory's coordinates.

\begin{figure}[H]
\includegraphics[width= \linewidth]{figs/history_ffn.pdf}
    \caption{Training history of the FFNN for the prediction of Lorenz attractor. $y$-axis is shown as the MAE divided by the average of the trajectory's coordinates.}
    \label{fig:history_ffn}
\end{figure}

\figref{fig:ffn_lorenz} decomposes the coordinates of the trajectory used for testing the predictions, giving a more clear visualization than the 3D representation. 

\begin{figure}[H]
\includegraphics[width= \linewidth]{figs/lorenz_ffn.pdf}
    \caption{Testing versus predicted trajectories decomposed by coordinates for the FFNN.}
    \label{fig:ffn_lorenz}
\end{figure}


\subsubsection{Building an RNN with TensorFlow}\label{subsubsection:RNN_TF}

Aiming at investigating the RNN variants at their best, we start a series of basic hyperparameters gridsearch. More specifically, we begin investigating how the accuracy of the stable spiral predictions changes for different hyperparameters. In that sense, \figref{fig:hyperparams_vanilla_spiral} shows the gridsearch of the number of epochs of training versus the length of the sequence the network is retroactively trained on for each step - the \emph{look-back} parameter.


\begin{figure}
    \centering
\includegraphics[width= \linewidth]{figs/sprial_gridsearch_lenght_epochs_vanilla.pdf}
    \caption{Gridsearch of look-back and epochs for the vanilla RNN trained on the stable spiral trajectory.}
    \label{fig:hyperparams_vanilla_spiral}
\end{figure}


Similarly, we can also visualize, in figure \figref{fig:hyperparams_lstm_spiral}, the accuracy of predictions for the same spiral train and test set, but trained on an LSTM implementation.

\begin{figure}
    \centering
\includegraphics[width= \linewidth]{figs/sprial_gridsearch_lenght_epochs_lstm.pdf}
    \caption{Gridsearch of look-back and epochs for the LSTM trained on the stable spiral trajectory.}
    \label{fig:hyperparams_lstm_spiral}
\end{figure}


Changing now the data set, we display in \figref{fig:hyperparams_vanilla_lorenz} and \figref{fig:hyperparams_lstm_lorenz} how the Lorenz attractor predictions performance vary with the look-back and number of epochs trained on. In an analogy to the stable spiral investigation, while the former shows the mean squared error for the vanilla model, the latter shows the loss on the LSTM implementation. Across parameters, This set of values, for the vanilla implementation has an average MSE of $0.35$ and variance of $0.31$. For
the LSTM implementation, the average MSE was of $0.024$ with $2.2 \cdot 10^{-5}$ variance. 

\begin{figure}
    \centering
\includegraphics[width= \linewidth]{figs/gridsearch_lenght_epochs_vanilla.pdf}
    \caption{Gridsearch of look-back and epochs for the
vanilla RNN trained on the Lorenz attractor trajectory. Average MSE of $0.35$ and variance of $0.31$}
    \label{fig:hyperparams_vanilla_lorenz}
\end{figure}

\begin{figure}
    \centering
\includegraphics[width= \linewidth]{figs/gridsearch_lenght_epochs_lstm.pdf}
    \caption{Gridsearch of look-back and epochs for the
LSTM trained on the Lorenz attractor trajectory. This set of values has an average of $0.024$ and variance of $2.2 \cdot 10^{-5}$}
    \label{fig:hyperparams_lstm_lorenz}
\end{figure}



\subsubsection{Comparing the networks}

Now that some base parameters were set, we can more fairly compare them. The trajectories for the predictions of the spiral can be seen in \figref{fig:rnn_spiral}, where we display the prediction of both models for the best LSTM parameters. The RMSE for these trajectories of the plot were $0.006$ for the Vanilla and $0.0005$ for the LSTM.

\begin{figure}[H]
\includegraphics[width= \linewidth]{figs/spiral_rnn.pdf}
    \caption{Here we display the predicted trajectories for both LSTM and Vanilla models for the best LSTM hyperparameters.}
    \label{fig:rnn_spiral}
\end{figure}

We now turn to the more intricate problem of predicting the time-series behavior of the Lorenz attractor. \figref{fig:rnn_lorenz}  gives a not-so-informative illustration of the predicted trajectories for the LSTM model versus the simple RNN. The mean-squared errors in this case were of $0.02$ for the LSTM versus $0.03$.

A perhaps mode clear illustration is given by the graph contained in \figref{fig:rnn_coordinate_errors_lorenz}. Here we display the cumulative absolute error of the coordinate's predictions along the time-steps


\begin{figure}[H]
\includegraphics[width= \linewidth]{figs/lorenz_rnn.pdf}
    \caption{Predictions for the Lorenz attractor trajectories. Comparison between LSTM and Vanilla network implementations.}
    \label{fig:rnn_lorenz}
\end{figure}


\begin{figure}[H]
\includegraphics[width= \linewidth]{figs/lorenz_rnn_lstm_vanilla_coordinates_errors.pdf}
    \caption{Sum of the coordinate's absolute errors for the LSTM and Vanilla RNNs for the predictions contained in \figref{fig:rnn_lorenz}.}
    \label{fig:rnn_coordinate_errors_lorenz}
\end{figure}


\subsubsection{Adding physics to the loss function}

The same results obtained for the comparison of the LSTM versus Vanilla implementation of the network can be repeated, now with the added physics-informed loss function term. The typical side-by-side predictions of the trajectories can be seen in \figref{fig:rnn_pi_lorenz}. For this simulation, we had mean squared errors of $0.02$ (0.018) for the naive LSTM model versus $0.01$ (0.013) for the physics-informed LSTM model.

\begin{figure}[H]
\includegraphics[width= \linewidth]{figs/lorenz_LSTMPI_rnn.pdf}
    \caption{Comparing the predictions of Lorenz trajectories for the LSTM with and without an added physics-informed term to the loss function.}
    \label{fig:rnn_pi_lorenz}
\end{figure}


Additionally, we can again study how the sum of the coordinate's errors evolves throughout the time evolution of the system in \figref{fig:lorenz_rnn_lstm_lstmPI_coordinates_errors}. 

\begin{figure}[H]
\includegraphics[width= \linewidth]{figs/lorenz_rnn_lstm_lstmPI_coordinates_errors.pdf}
    \caption{Sum of the absolute errors for all coordinates of the LSTM and physics-informed LSTM model for each time-step. These errors are from the trajectories displayed in \figref{fig:rnn_pi_lorenz}.}
    \label{fig:lorenz_rnn_lstm_lstmPI_coordinates_errors}
\end{figure}


To understand the effects of the addition of the physics-informed penalty to the training mean squared error values, we display \figref{fig:loghist_lorenz_LSTMPI_rnn}. This figure shows the log of the evaluation metric while the model is being trained, and compares the LSTM networks with and without PI penalty. 

\begin{figure}[H]
\includegraphics[width= \linewidth]{figs/loghist_lorenz_LSTMPI_rnn.pdf}
    \caption{Log of the mean squared error of predictions throughout the training process.}
    \label{fig:loghist_lorenz_LSTMPI_rnn}
\end{figure}


Finally, a brief overview of the models and obtained mean squared error scores for the Lorenz trajectories can be seen in \tabref{tab:compare}.



\begin{table}[]
\begin{tabular}{ccccc}
\hline
Learning Rate & Epochs & Look-back & MSE    & Model   \\ \hline
 $10^{-3}$     & $500$  & $2$       & $0.03$ & Vanilla \\
 $10^{-3}$     & $500$  & $2$       & $0.02$ & LSTM    \\
 $10^{-3}$     & $500$  & $2$       & $0.01$ & LSTM PI
\end{tabular}
\caption{Overview of significant results in comparison of the models for the Lorenz attractor prediction. All of the models here used Adam as optimizer.}
\label{tab:compare}
\end{table}










