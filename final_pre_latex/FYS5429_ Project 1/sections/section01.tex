\section{Introduction}\label{introduction}

In 1963, the meteorologist and mathematician E. N. Lorenz unveiled an extraordinary system. By significantly reducing the complexity of a model for weather prediction, he came across a three-dimensional differential equation that demonstrated a phenomenon known as "deterministic chaos".

Upon employing certain constants, an extensive collection of solutions gravitate towards a set resembling the shape of a butterfly - hence dubbed the 'Lorenz Attractor'. This particular type of behavior is categorized as "chaos" \cite{zhang2022}.

The difficulty created by this set among many other sets of coupled differential equations is that they have no analytical solutions. Recently, advancements in machine learning and, more specifically, recurrent neural networks (RNNs), have provided us with a new way through which we can investigate this dynamic system.

In this report, we studied different ways to predict a time sequence with various complexities. Using time sequences generated by standard fourth-order Runge-Kutta (RK4) methods, we started by attempting to predict future steps using a standard Feed Forward Neural Network, then a simple RNN, and thereafter, LSTM RNNs with and without an added physics-informed loss function. We then compared their behaviors and explained why LSTM RNNs are best suited for studying this time series. Lastly, we also explored a less chaotic system, namely the stable spiral. The results proved to be very interesting.

The goal of this report is to provide our findings on how RNNs predict time series and chaotic systems and explain the difficulties and potential solutions, in an attempt to generate new insights.

In section \ref{methods}, we present an overview of RNNs and their basic ingredients, such as recurrent connections, hidden states, activation functions, and backpropagation through time. Subsequently, we present the physical model of Lorenz attractors to explain how we generated the data set via simulations and discuss how chaotic systems are hard to learn for an ML algorithm. We start section \ref{results}, by showing that a regular FFNN is unable to learn the proper dynamics of the chaotic motion for a satisfactory trajectory extrapolation.

Section \ref{discussion} analyses the obtained results, comparing the implemented methods and their ability to extract data from the model, noting possible future improvements.
In Section \ref{conclusion}, we conclude with a summary of what was learned from the several results and methods. 


